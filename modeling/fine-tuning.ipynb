{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/space/hotel/taile/miniconda3/envs/brandon_proj/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.utils.data\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel, EvalPrediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./dataset/GLAMI-1M-train-dataset/GLAMI-1M-train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>geo</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>category</th>\n",
       "      <th>category_name</th>\n",
       "      <th>label_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cz</td>\n",
       "      <td>Casio Collection MTP-1259D-7BEF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>645</td>\n",
       "      <td>mens-watches</td>\n",
       "      <td>custom-tag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>cz</td>\n",
       "      <td>Casio Collection MTP-1303D-1AVEF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>645</td>\n",
       "      <td>mens-watches</td>\n",
       "      <td>custom-tag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>cz</td>\n",
       "      <td>Hot Diamonds Náušnice Eternity Interlocking St...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>468</td>\n",
       "      <td>womens-earrings</td>\n",
       "      <td>custom-tag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>cz</td>\n",
       "      <td>Shepherd Papuče ANTON</td>\n",
       "      <td>Shepherd Papuče ANTON Béžová. K dispozici v pá...</td>\n",
       "      <td>40328</td>\n",
       "      <td>mens-boots</td>\n",
       "      <td>custom-tag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>cz</td>\n",
       "      <td>Vestis Dámský župan Milano 3130 - dle obrázku - S</td>\n",
       "      <td>Dámský župan Milano Vestis s proužky. Župan s ...</td>\n",
       "      <td>437</td>\n",
       "      <td>womens-bathrobes</td>\n",
       "      <td>custom-tag</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id  image_id geo                                               name  \\\n",
       "0        0         0  cz                    Casio Collection MTP-1259D-7BEF   \n",
       "1        3         3  cz                   Casio Collection MTP-1303D-1AVEF   \n",
       "2        5         5  cz  Hot Diamonds Náušnice Eternity Interlocking St...   \n",
       "3        7         7  cz                              Shepherd Papuče ANTON   \n",
       "4        8         8  cz  Vestis Dámský župan Milano 3130 - dle obrázku - S   \n",
       "\n",
       "                                         description  category  \\\n",
       "0                                                NaN       645   \n",
       "1                                                NaN       645   \n",
       "2                                                NaN       468   \n",
       "3  Shepherd Papuče ANTON Béžová. K dispozici v pá...     40328   \n",
       "4  Dámský župan Milano Vestis s proužky. Župan s ...       437   \n",
       "\n",
       "      category_name label_source  \n",
       "0      mens-watches   custom-tag  \n",
       "1      mens-watches   custom-tag  \n",
       "2   womens-earrings   custom-tag  \n",
       "3        mens-boots   custom-tag  \n",
       "4  womens-bathrobes   custom-tag  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_category_name(category_name: str):\n",
    "    category_name = category_name.replace(\"-\", \" \")\n",
    "    category_name = category_name.replace(\"womens\", \"women's\").replace(\"women s\", \"women's\")\n",
    "    category_name = category_name.replace(\"mens\", \"men's\").replace(\"men s\", \"men's\")\n",
    "    category_name = category_name.replace(\"boys\", \"boy's\").replace(\"boy s\", \"boy's\")\n",
    "    category_name = category_name.replace(\"girls\", \"girl's\").replace(\"girl s\", \"girl's\")\n",
    "    category_name = category_name.replace(\"and\", \"or\")\n",
    "    category_name = category_name.replace(\"t shirts\", \"t-shirts\")\n",
    "    return category_name\n",
    "\n",
    "def preprocess_category_names_in_batch(category_names):\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(preprocess_category_name, category_names))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"processed_category_name\"] = None\n",
    "batch_size = 1000\n",
    "for start in range(0, len(df), batch_size):\n",
    "    end = min(start + batch_size, len(df))\n",
    "    batch_category_names = df[\"category_name\"][start:end]\n",
    "    batch_processed_names = preprocess_category_names_in_batch(batch_category_names)\n",
    "    df.iloc[start:end, df.columns.get_loc(\"processed_category_name\")] = batch_processed_names\n",
    "# Drop the original category name column\n",
    "df = df.reset_index().rename(columns={'index': 'id'})\n",
    "df = df.drop(columns=[\"category_name\"])[[\"processed_category_name\", \"id\", \"image_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_category_name</th>\n",
       "      <th>id</th>\n",
       "      <th>image_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>men's watches</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>men's watches</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>women's earrings</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>men's boots</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>women's bathrobes</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  processed_category_name  id  image_id\n",
       "0           men's watches   0         0\n",
       "1           men's watches   1         3\n",
       "2        women's earrings   2         5\n",
       "3             men's boots   3         7\n",
       "4       women's bathrobes   4         8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the data by the `processed category name` and get the count of each category\n",
    "category_counts = df.groupby(\"processed_category_name\").agg(\n",
    "    count=(\"id\", \"size\"),\n",
    "    ids=(\"id\", lambda x: list(x))\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_category_name</th>\n",
       "      <th>count</th>\n",
       "      <th>ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baby accessories</td>\n",
       "      <td>1200</td>\n",
       "      <td>[6059, 8460, 8461, 8811, 10357, 12939, 12940, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>baby clothing</td>\n",
       "      <td>27896</td>\n",
       "      <td>[14957, 14958, 14963, 14964, 14977, 14984, 149...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>baby shoes</td>\n",
       "      <td>14577</td>\n",
       "      <td>[37415, 37942, 38129, 38884, 38941, 40618, 406...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bathroom</td>\n",
       "      <td>2318</td>\n",
       "      <td>[841, 842, 2138, 2329, 2330, 2331, 2332, 2335,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bed linen</td>\n",
       "      <td>5254</td>\n",
       "      <td>[361, 362, 363, 388, 389, 676, 1582, 1738, 230...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>women's undershirts</td>\n",
       "      <td>773</td>\n",
       "      <td>[4867, 11589, 12103, 12198, 12521, 37824, 3817...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>women's vests</td>\n",
       "      <td>3149</td>\n",
       "      <td>[9589, 12081, 12883, 18566, 21051, 21508, 2175...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>women's wallets</td>\n",
       "      <td>5442</td>\n",
       "      <td>[107, 109, 139, 345, 401, 412, 430, 435, 692, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>women's watches</td>\n",
       "      <td>3062</td>\n",
       "      <td>[3691, 4070, 4156, 4160, 4163, 4166, 4167, 416...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>women's windbreakers</td>\n",
       "      <td>2282</td>\n",
       "      <td>[3491, 5499, 5500, 5501, 8795, 12184, 14735, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    processed_category_name  count  \\\n",
       "0          baby accessories   1200   \n",
       "1             baby clothing  27896   \n",
       "2                baby shoes  14577   \n",
       "3                  bathroom   2318   \n",
       "4                 bed linen   5254   \n",
       "..                      ...    ...   \n",
       "186     women's undershirts    773   \n",
       "187           women's vests   3149   \n",
       "188         women's wallets   5442   \n",
       "189         women's watches   3062   \n",
       "190    women's windbreakers   2282   \n",
       "\n",
       "                                                   ids  \n",
       "0    [6059, 8460, 8461, 8811, 10357, 12939, 12940, ...  \n",
       "1    [14957, 14958, 14963, 14964, 14977, 14984, 149...  \n",
       "2    [37415, 37942, 38129, 38884, 38941, 40618, 406...  \n",
       "3    [841, 842, 2138, 2329, 2330, 2331, 2332, 2335,...  \n",
       "4    [361, 362, 363, 388, 389, 676, 1582, 1738, 230...  \n",
       "..                                                 ...  \n",
       "186  [4867, 11589, 12103, 12198, 12521, 37824, 3817...  \n",
       "187  [9589, 12081, 12883, 18566, 21051, 21508, 2175...  \n",
       "188  [107, 109, 139, 345, 401, 412, 430, 435, 692, ...  \n",
       "189  [3691, 4070, 4156, 4160, 4163, 4166, 4167, 416...  \n",
       "190  [3491, 5499, 5500, 5501, 8795, 12184, 14735, 1...  \n",
       "\n",
       "[191 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load original CLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CLIP model and processor from Hugging Face\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# Set `device_map` to `cuda` to ignore the error `CUBLAS_STATUS_EXECUTION_FAILED` on multi-GPU machines\n",
    "# See: https://forums.developer.nvidia.com/t/cublas-status-execution-failed/27370\n",
    "model = CLIPModel.from_pretrained(model_id, device_map=device)\n",
    "processor = CLIPProcessor.from_pretrained(model_id, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, processor):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the processed category name (text) and image path\n",
    "        category = self.dataframe.iloc[idx]['processed_category_name']\n",
    "        image_id = self.dataframe.iloc[idx]['image_id']\n",
    "        image_path = os.path.join(self.image_dir, f\"{image_id}.jpg\")\n",
    "\n",
    "        # Open the image\n",
    "        image = Image.open(image_path).resize((224, 224)).convert(\"RGB\")\n",
    "\n",
    "        query = f'a photo of the {category}'\n",
    "\n",
    "        # Process the text and image into tensors\n",
    "        inputs = self.processor(text=query, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "        # Remove the batch dimension from inputs as DataLoader adds it\n",
    "        inputs = {key: val.squeeze(0) for key, val in inputs.items()}\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = './dataset/GLAMI-1M-train-dataset/images'\n",
    "dataset = FashionDataset(df, image_dir, processor)\n",
    "\n",
    "# Split the dataset\n",
    "train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "val_size = len(dataset) - train_size  # 20% for validation\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration for Parameter-Efficient Fine-Tuning (PEFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT Configuration for Low-Rank Adaptation (LoRA)\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"k_proj\", \"v_proj\", \"q_proj\", \"out_proj\"],\n",
    "    base_model_name_or_path=model_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in original model: 151277313\n",
      "Trainable parameters in original model: 151277313\n",
      "Trainable parameters after applying LoRA: 1966080\n",
      "Percentage of parameters being trained: 1.30%\n"
     ]
    }
   ],
   "source": [
    "# Count original model parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Initialize LoRA model\n",
    "lora_model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Count parameters after applying LoRA\n",
    "lora_trainable_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\n",
    "\n",
    "# Print the details\n",
    "print(f\"Total parameters in original model: {total_params}\")\n",
    "print(f\"Trainable parameters in original model: {trainable_params}\")\n",
    "print(f\"Trainable parameters after applying LoRA: {lora_trainable_params}\")\n",
    "print(f\"Percentage of parameters being trained: {100 * lora_trainable_params / total_params:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on CPU: False\n",
      "Training with FP16: True\n"
     ]
    }
   ],
   "source": [
    "# Define the training arguments\n",
    "use_cpu = True if device.type == \"cpu\" else False\n",
    "use_fp16 = True if torch_dtype == torch.float16 else False\n",
    "print(f\"Training on CPU: {use_cpu}\")\n",
    "print(f\"Training with FP16: {use_fp16}\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",             # Directory to save checkpoints and results\n",
    "    per_device_train_batch_size=512,     # Batch size for training (adjust based on your GPU memory)\n",
    "    per_device_eval_batch_size=512,      # Batch size for evaluation\n",
    "    num_train_epochs=5,                 # Number of epochs to train for\n",
    "    logging_dir=\"./logs\",               # Directory for storing logs\n",
    "    logging_steps=10,                   # Log after every 10 steps\n",
    "    load_best_model_at_end=True,        # Load the best model at the end based on evaluation metrics\n",
    "    save_total_limit=2,                 # Limit the number of saved checkpoints\n",
    "    eval_strategy=\"epoch\",              # Evaluate after every epoch\n",
    "    save_strategy=\"epoch\",              # Save checkpoint after every epoch\n",
    "    learning_rate=5e-5,                 # Learning rate (adjust based on your model size and dataset)\n",
    "    report_to=\"none\",                   # Avoid reporting to external services like WandB, TensorBoard\n",
    "    fp16=use_fp16,                      # Enable mixed-precision training (if your hardware supports it)\n",
    "    gradient_accumulation_steps=2,      # Accumulate gradients over multiple steps before updating\n",
    "    use_cpu=use_cpu                     # Use CPU for training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity\n",
    "def cosine_similarity(embeddings_1, embeddings_2):\n",
    "    return torch.matmul(embeddings_1, embeddings_2.T)\n",
    "\n",
    "# Compute retrieval accuracy\n",
    "def compute_metrics(eval_pred: EvalPrediction):\n",
    "    # Separate image and text embeddings from eval_pred\n",
    "    image_embeddings, text_embeddings = eval_pred.predictions\n",
    "    batch_size = image_embeddings.shape[0]\n",
    "\n",
    "    # Normalize embeddings\n",
    "    image_embeddings = torch.nn.functional.normalize(image_embeddings, dim=-1)\n",
    "    text_embeddings = torch.nn.functional.normalize(text_embeddings, dim=-1)\n",
    "\n",
    "    # Compute similarity matrix (cosine similarity)\n",
    "    similarity_matrix = cosine_similarity(image_embeddings, text_embeddings)\n",
    "    \n",
    "    # Ground truth: diagonal contains correct pairs (as we use pairs from the dataset)\n",
    "    ground_truth = torch.arange(batch_size)\n",
    "\n",
    "    # Evaluate image-to-text and text-to-image retrieval\n",
    "    image_to_text_accuracy = (similarity_matrix.argmax(dim=-1) == ground_truth).float().mean().item()\n",
    "    text_to_image_accuracy = (similarity_matrix.argmax(dim=0) == ground_truth).float().mean().item()\n",
    "\n",
    "    return {\n",
    "        \"image_to_text_accuracy\": image_to_text_accuracy,\n",
    "        \"text_to_image_accuracy\": text_to_image_accuracy\n",
    "    }\n",
    "\n",
    "def compute_loss(model, inputs, return_outputs=False):\n",
    "    # Forward pass\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # Extract logits for image and text\n",
    "    logits_per_image = outputs.logits_per_image\n",
    "    logits_per_text = outputs.logits_per_text\n",
    "    \n",
    "    # Calculate the contrastive loss (cross-entropy)\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Labels are the diagonal of the similarity matrix (0 to batch_size-1)\n",
    "    labels = torch.arange(logits_per_image.size(0), device=logits_per_image.device)\n",
    "\n",
    "    # Compute the contrastive loss (sum of image-to-text and text-to-image)\n",
    "    loss_img_to_text = loss_fct(logits_per_image, labels)\n",
    "    loss_text_to_img = loss_fct(logits_per_text, labels)\n",
    "    loss = (loss_img_to_text + loss_text_to_img) / 2\n",
    "\n",
    "    return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    keys = batch[0].keys()\n",
    "    collated = {}\n",
    "\n",
    "    for key in keys:\n",
    "        if key == 'pixel_values':\n",
    "            # Assuming all images are already the same size\n",
    "            collated[key] = torch.stack([item[key] for item in batch])\n",
    "        elif key == 'input_ids' or key == 'attention_mask':\n",
    "            # Pad sequences to the maximum length in the batch\n",
    "            collated[key] = pad_sequence([item[key] for item in batch], batch_first=True, padding_value=0)\n",
    "        else:\n",
    "            # For any other keys, just stack them\n",
    "            collated[key] = torch.stack([item[key] for item in batch])\n",
    "\n",
    "    return collated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        return compute_loss(model, inputs, return_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=lora_model,                       # LoRA model\n",
    "    args=training_args,                     # Training arguments defined above\n",
    "    train_dataset=train_dataset,            # Training dataset\n",
    "    eval_dataset=val_dataset,               # Evaluation dataset\n",
    "    compute_metrics=compute_metrics,        # Custom function to compute accuracy\n",
    "    data_collator=custom_collate_fn         # Custom collate function to handle mixed data types\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brandon_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
